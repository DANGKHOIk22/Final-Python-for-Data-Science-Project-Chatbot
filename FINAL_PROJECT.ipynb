{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "TÊN NHÓM: NHÓM 19\n",
        "\n",
        "THÀNH VIÊN\n",
        "\n",
        "22280048: THÁI ANH KHOA\n",
        "\n",
        "22280049: VŨ ĐĂNG KHÔI\n",
        "\n",
        "22280051: NGUYỄN VĂN KINH\n",
        "\n",
        "22280055: LÊ THÀNH NAM"
      ],
      "metadata": {
        "id": "PjpGRg4VO0zo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQcG0fHOi-SO"
      },
      "source": [
        "## I. LLM integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvOwuKNoNkHs"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0wV9227JGqr"
      },
      "outputs": [],
      "source": [
        "genai.configure(api_key= \"AIzaSyDd6D4PnEuUEhu4WwuuoHPX_bICDaZ9570\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4txUodI7jHXu"
      },
      "source": [
        "### 1.Single Text Translation:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dùng LLM để dịch một câu bất kỳ sang một tiếng nào đó, mô hình sử dụng là gemini-1.5-flash"
      ],
      "metadata": {
        "id": "b_MGec-OQLGj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dTwyj2TREo90"
      },
      "outputs": [],
      "source": [
        "def translate_single_text(json_file):\n",
        "  # API\n",
        "  model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "  # prompt\n",
        "  prompt = (\n",
        "           'You are provided with a text type json, including key \"text\" and key \"dest_language\" like this: {\"text\": \"text here\", \"dest_language\": \"abc\"}.'\n",
        "           'Your job is to be a translator application, translate \"text\" to the suitable \"dest_language\".'\n",
        "           'Your response must be a plain text, no adding extra text or any elements.'\n",
        "           'For example, text: \"Hello, I am Huong\" with dest_language:\"vi\" should be translated to \"Xin chào, tôi là Huong\".'\n",
        "           'If text has already been in dest_language, return it raw, not adding any extra text.'\n",
        "           'If there is a name in text, the name should be returned unedited, for example \"Anh\" should be returned \"Anh\" and not \"Ánh or Ảnh\"'\n",
        "           'Sometimes words in \"text\" might appear to be more in different language, try to detect which words in what language and translate it to \"dest_language\", for example \"One hai ba four\" translated to \"Một hai ba bốn\".'\n",
        "           'Remember that the response must be in dest_language'\n",
        "           'This is your file, please translate it carefully: '\n",
        "  )\n",
        "  response = model.generate_content(prompt + str(json_file))\n",
        "  result = response.text\n",
        "\n",
        "  return result.replace(\"\\n\",\"\")\n",
        "\n",
        "json_1 ={\n",
        "    'text': 'Hello, tôi is Khoa, tôi thích ăn orange',\n",
        "    'dest_language': 'vi'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "B1H0DFNbKgrI",
        "outputId": "4f7b9f34-68a7-4d73-fe00-ccf8c5ae72ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Xin chào, tôi là Khoa, tôi thích ăn cam\n"
          ]
        }
      ],
      "source": [
        "print(translate_single_text(json_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlQSAdK7jTcm"
      },
      "source": [
        "### 2.Multiple Texts Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dùng LLM để dịch một mảng các bất kỳ sang một tiếng nào đó, mô hình sử dụng là gemini-1.5-flash"
      ],
      "metadata": {
        "id": "_yeJsU5UQs_a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DreIQX9DcvX"
      },
      "outputs": [],
      "source": [
        "json_2 = {\n",
        "    'text': ['Hello','I am Khoa','Tôi là sinh viên','How are you going'],\n",
        "    'dest_language': 'vi'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWItsktcIgL1"
      },
      "outputs": [],
      "source": [
        "def translate_multiple_text(json_file):\n",
        "  # API\n",
        "  model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "  # prompt\n",
        "  prompt = (\n",
        "           'You are provided with a object type json with key \"text\" and key \"dest_language\", like this: {\"text\": [\"a\", \"b\",...], \"dest_language\": \"c\"}.'\n",
        "           'Your job is to be a translator application,translate the elemement with a text type in \"text\" to the approriate \"dest_language\".'\n",
        "           'Your response must only be a JSON array of translated text-type elements'\n",
        "           'For example, \"text\":[\"Hello\",\"Sorry\",\"Tôi là ma\",\"How old are you ?\"] with \"dest_language\":\"vi\" must return [\"Xin chào\",\"Xin lỗi\",\"Tôi là ma\",\"Bạn bao nhiêu tuổi ?\"]'\n",
        "           'If the elements in text are already in dest_language, return it raw, not adding any extra element.'\n",
        "           'Sometimes elements in text might be in different language, remember to detect which elements in what language and translate it to \"dest_language\", \\\n",
        "            for example \"[\"One năm\",\"hai\",\"ba\",\"four\"] translated to [\"Một năm\",\"hai\",\"ba\",\"bốn\"].'\n",
        "           'Sometimes words in \"text\" might appear to be more in different language, try to detect which words in what language and translate it to \"dest_language\"'\n",
        "           'If there a name in text, the name should be returned unedited, for example \"Anh\" should be returned \"Anh\" and not \"Ánh or Ảnh\"'\n",
        "           'Remember that every elements in the response must be in dest_language.'\n",
        "           'Remember that every words in the response must be in dest_language.'\n",
        "           'Remember the respond must be in JSON array'\n",
        "           'Remember all the double quotes \" in JSON array response must be transformed to single quote in the response '\n",
        "           'This is your file, please translate it carefully: '\n",
        "  )\n",
        "  response = model.generate_content(prompt + str(json_file))\n",
        "  result = response.text\n",
        "\n",
        "  start = result.find('[')\n",
        "  end = result.rfind(']') + 1\n",
        "\n",
        "\n",
        "  return result[start:end]\n",
        "\n",
        "json_2 = {\n",
        "    'text': ['Hello','I am Huong','Tôi am sinh viên','How are you doing','Tôi thích ăn strawberry'],\n",
        "    'dest_language': 'vi'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24As29hbkfzV"
      },
      "outputs": [],
      "source": [
        "genai.configure(api_key= \"AIzaSyDd6D4PnEuUEhu4WwuuoHPX_bICDaZ9570\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "M6nAUFdkh34S",
        "outputId": "eee1a130-b2c2-4601-aff8-c586230be29b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Xin chào', 'Tôi là Huong', 'Tôi là sinh viên', 'Bạn khỏe không', 'Tôi thích ăn dâu tây']\n"
          ]
        }
      ],
      "source": [
        "print(translate_multiple_text(json_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OH1msfyjf62"
      },
      "source": [
        "## II. Chatbot Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgnbYUBfmNd3"
      },
      "source": [
        "### 1.Data Access and Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crawl Data:\n",
        "\n",
        "Sử dụng Selenium để load toàn bộ nội dung của HTML\n",
        "\n",
        "Dùng Beautiful Soup để chiết xuất ra các thông tin của từng mục\n",
        "\n",
        "Sau đó lập chỉ mục cho data theo từng mục như trên\n",
        "\n",
        "Dùng model all-MiniLM-L6-v2 để embedding data"
      ],
      "metadata": {
        "id": "z099YPKORc0G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJMM80oRknBn"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.common.by import By\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from bs4 import BeautifulSoup\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Xo0zoTobknuO"
      },
      "outputs": [],
      "source": [
        "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
        "\n",
        "url = f'https://www.presight.io/privacy-policy.html'\n",
        "driver.get(url)\n",
        "\n",
        "html_content = driver.page_source\n",
        "\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajDm4RPEwFCM"
      },
      "outputs": [],
      "source": [
        "soup = BeautifulSoup(html_content, 'html.parser')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fnty15GVwJpt"
      },
      "outputs": [],
      "source": [
        "tags = []\n",
        "contents = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRkxGV2mwLAD"
      },
      "outputs": [],
      "source": [
        "overview = soup.find('p',class_= 'chakra-text css-0').text.strip()\n",
        "tags.append('Overview')\n",
        "contents.append(overview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yilac6mdwMqj"
      },
      "outputs": [],
      "source": [
        "latest_update = soup.find('h2',class_ = 'chakra-heading css-18j379d').text.strip()\n",
        "tags.append('Latest Update')\n",
        "contents.append(latest_update)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAxCFSRnwOKy"
      },
      "outputs": [],
      "source": [
        "html_content = soup.find('div', class_='css-fugq39')\n",
        "all_chunk = html_content.find_all('div', class_='chakra-stack css-o5l3sd')\n",
        "\n",
        "for chunk in all_chunk:\n",
        "    content_chunk = chunk.get_text('\\n',strip=True)\n",
        "    tags.append(content_chunk.split('\\n', 1)[0])\n",
        "    contents.append(content_chunk.split('\\n', 1)[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3DpOQYoATIm",
        "outputId": "3b3423d7-feae-48c6-fc27-405b994ec609"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['At Presight, we are committed to protecting the privacy of our customers and visitors to our website. This Privacy Policy explains how we collect, use, and disclose information about our customers and visitors.',\n",
              " 'Last updated 15 Sep 2023',\n",
              " 'We collect several different types of information for various purposes to provide and improve our Service to you.',\n",
              " 'Personal Data\\nWhile using our Service, we may ask you to provide us with certain personally identifiable information that can be used to contact or identify you (\"Personal Data\"). Personally identifiable information may include, but is not limited to:\\nEmail address\\nFirst name and last name\\nPhone number\\nAddress, State, Province, ZIP/Postal code, City\\nCookies and Usage Data\\nUsage Data\\nWe may also collect information that your browser sends whenever you visit our Service or when you access the Service by or through a mobile device (\"Usage Data\"). This Usage Data may include information such as your computer\\'s Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that you visit, the time and date of your visit, the time spent on those pages, unique device identifiers, and other diagnostic data.',\n",
              " 'Presight uses the collected data for various purposes:\\nTo provide and maintain our Service\\nTo notify you about changes to our Service\\nTo allow you to participate in interactive features of our Service when you choose to do so\\nTo provide customer support\\nTo gather analysis or valuable information so that we can improve our Service\\nTo monitor the usage of our Service\\nTo detect, prevent and address technical issues',\n",
              " 'As personal information is collected, you will be asked to confirm that your information is correct prior to submitting it to Presight.',\n",
              " 'Accessing Your Personal Information\\nYou have the right to access all of your personal information that we hold. Through the application, you can correct, amend, or append your personal information by logging into the application and navigating to your settings and profile.\\nAutomated Edit Checks\\nPresight employs automated edit checks to ensure that data entry fields are completed properly when collecting personal information. These edit checks help maintain data integrity and accuracy. You are encouraged to provide complete and valid information to ensure the smooth processing of their personal data.',\n",
              " 'We may disclose your application data to third-party service providers who help us provide our services such as Datadog, AWS, Google Cloud and Google Workspace. We may also disclose your information in response to a legal request, such as a subpoena or court order, or to protect our rights or the rights of others.',\n",
              " 'Your personal data will not be subject to sharing, transfer, rental or exchange for the benefit of third parties, including AI models.',\n",
              " 'In all cases when users authenticate the platform to Google Workspace, the following applies:\\nWe do not retain or use Google User Data to develop, improve, or train generalized/non-personalized AI and/or ML models.\\nWe do not use Google Workspace APIs to develop, improve, or train generalized/non-personalized AI and/or ML models.\\nWe do not transfer Google User Data to third-party AI tools for the purpose of developing, improving, or training generalized or non-personalized AI and/or ML models.',\n",
              " 'All data is encrypted both in transit and at rest, using industry-standard encryption methods.\\nWe regularly perform security audits and vulnerability assessments to ensure the safety of our platform and the data stored within it.\\nOur employees are trained on best practices for data security, and access to customer data is restricted on a need-to-know basis.',\n",
              " 'Customer data is retained for as long as the account is in active status. Data enters an “expired” state when the account is voluntarily closed. Expired account data will be retained for 60 days. After this period, the account and related data will be removed.',\n",
              " 'We are committed to maintaining the quality and accuracy of the personal information we collect and process.\\nWe rely on data subjects to provide accurate and up-to-date information.\\nData subjects have the responsibility to inform us of any changes or inaccuracies in their personal data.\\nIf you believe that any information we hold about you is inaccurate, incomplete, or outdated, please contact us promptly to rectify the information.',\n",
              " 'We regularly monitor its data processing activities to ensure compliance with this privacy policy and applicable data protection laws.\\nIn the event of a data breach or any unauthorized access to your personal information, we will notify you and the appropriate authorities as required by law.\\nWe committed to cooperating with data protection authorities and complying with their advice and decisions regarding data protection and privacy matters.',\n",
              " 'We use cookies to enhance your experience on our website. You can control the use of cookies through your web browser settings.',\n",
              " 'Our website may contain links to third-party websites. We are not responsible for the privacy practices or content of those websites.',\n",
              " 'We may update this Privacy Policy from time to time. The updated Privacy Policy will be posted on our website.',\n",
              " 'If you have any questions about this Privacy Policy, please contact us through the customer portal or by email at\\npresight@presight.io.',\n",
              " \"We commit to only use personal information for the purposes identified in the entity's privacy policy.\"]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpcqnYkZCKwE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer,util\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "embeddings = model.encode(contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9YilcUAiMxr",
        "outputId": "f0d4a6b3-9863-4a7f-ea97-c654ec416912"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-0.06845757,  0.07021626, -0.00354795, ...,  0.04650678,\n",
              "        -0.00165939,  0.04551703],\n",
              "       [-0.10518023, -0.01589324,  0.05111733, ..., -0.07926523,\n",
              "        -0.03897299,  0.08186397],\n",
              "       [-0.04906534,  0.03099293, -0.04653648, ...,  0.02137542,\n",
              "        -0.07456707,  0.00753139],\n",
              "       ...,\n",
              "       [-0.0551081 , -0.00417258,  0.02536841, ..., -0.02517021,\n",
              "        -0.04174801,  0.00684592],\n",
              "       [-0.02871253,  0.01158661, -0.00069179, ..., -0.05492673,\n",
              "        -0.05053495,  0.05449985],\n",
              "       [-0.01027679,  0.09557909, -0.01535839, ...,  0.00629754,\n",
              "         0.02210543, -0.03520378]], dtype=float32)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3hRmxkMmG05"
      },
      "source": [
        "### 2.Chatbot Development"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiền xử lý dữ liệu\n",
        "- Lọc lấy từ và số\n",
        "- Chuyển thành chữ viết thường\n",
        "- Tách từ\n",
        "- Loại bỏ stop-words\n",
        "- Đưa các từ về định dạng gốc bằng Lemmatization\n",
        "- Cần tách các đoạn ra thành từng câu"
      ],
      "metadata": {
        "id": "BjbedaoWSZO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Phương pháp Indexing**\n",
        "\n",
        "Phương thức 1: Sử dụng model distilbert-inspec để lấy key-word trong query, ta đi tính toán score của từng câu bằng cách tính TF-IDF của key-word đó nhân với lại similarity của câu query với câu, và chia với (1 - trung bình similarity) của câu đó với các câu còn lại. Sau đó chuẩn hóa bằng minmaxScaler.\n",
        "\n",
        "Phương thức 2: Sử dụng những câu chưa bỏ stop-words, và mô hình bge-m3 để sử dụng hybrid-retrieval cho tìm kiếm theo ngữ nghĩa và theo tìm kiếm theo từ vựng:\n",
        "- Dense Retrieval: Hiểu được ý nghĩa tổng thể của câu\n",
        "- Sparse Retrieval: Dùng để so khớp sự chính xác giữa các từ trong câu truy vấn và câu trả lời\n",
        "- Multi-vector Retrieval: Giúp hiểu được nhiều khía cạnh chi tiết hơn của câu hoặc văn bản có nội dung phức tạp\n",
        "\n",
        "**Phương pháp lấy top-K**\n",
        "\n",
        "- Tính tổng giá trị score từ hai phương thức 1 và 2 cho tất cả các câu trong dữ liệu.\n",
        "- Sau đó tính trung bình và độ lệch chuẩn cho giá trị score. Lấy tất cả các câu có score lớn hơn \\mu\n",
        "-\n"
      ],
      "metadata": {
        "id": "pesRl7jkTnlz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKtV-aYeyiCT"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import regex as re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUh1Tailiy0o"
      },
      "source": [
        "#### Chatbot Development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TUUIJU7loXu-"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/FlagOpen/FlagEmbedding.git\n",
        "!cd FlagEmbedding\n",
        "!pip install -e .\n",
        "!pip install -U FlagEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zygG0BC9YMd5"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    TokenClassificationPipeline,\n",
        "    AutoModelForTokenClassification,\n",
        "    AutoTokenizer,\n",
        ")\n",
        "from transformers.pipelines import AggregationStrategy\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "from FlagEmbedding import BGEM3FlagModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "class KeyphraseExtractionPipeline(TokenClassificationPipeline):\n",
        "    def __init__(self, model, *args, **kwargs):\n",
        "        super().__init__(\n",
        "            model=AutoModelForTokenClassification.from_pretrained(model),\n",
        "            tokenizer=AutoTokenizer.from_pretrained(model),\n",
        "            *args,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def postprocess(self, all_outputs):\n",
        "        results = super().postprocess(\n",
        "            all_outputs=all_outputs,\n",
        "            aggregation_strategy=AggregationStrategy.FIRST,\n",
        "        )\n",
        "        return np.unique([result.get(\"word\").strip() for result in results])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCWh79hdaaje"
      },
      "outputs": [],
      "source": [
        "class MyModel():\n",
        "    def __init__(self, contents):\n",
        "        self.contents = contents\n",
        "        self.documents = []\n",
        "        self.extractor = KeyphraseExtractionPipeline(model=\"ml6team/keyphrase-extraction-distilbert-inspec\")\n",
        "        self.model_encoder = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)\n",
        "        self.single_contents = []\n",
        "        self.single_contents_BGE = []\n",
        "        self.embeddings = None\n",
        "        self.embeddings_BGE = None\n",
        "\n",
        "    @staticmethod\n",
        "    def text_processing(text, process_stopwords=True):\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "        text = text.lower()\n",
        "\n",
        "        word_tokens = word_tokenize(text)\n",
        "\n",
        "        if process_stopwords:\n",
        "            filtered_sentence = [w for w in word_tokens if w not in stop_words]\n",
        "        else:\n",
        "            filtered_sentence = word_tokens\n",
        "\n",
        "        filtered_sentence = [lemmatizer.lemmatize(w) for w in filtered_sentence]\n",
        "\n",
        "        return \" \".join(filtered_sentence)\n",
        "\n",
        "    def processor(self):\n",
        "        self.documents = []\n",
        "\n",
        "        for s in self.contents:\n",
        "            sentences = [sentence.strip() for sentence in re.split(r'\\. ', s) if sentence.strip()]\n",
        "            self.documents.extend(sentences)\n",
        "\n",
        "        for i in range(len(self.documents)):\n",
        "            self.single_contents.append(self.text_processing(self.documents[i]))\n",
        "            self.single_contents_BGE.append(self.text_processing(self.documents[i], process_stopwords=False))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_embeddings(self):\n",
        "        self.embeddings = self.model_encoder.encode(\n",
        "            self.single_contents,\n",
        "            return_dense=True,\n",
        "            return_sparse=True,\n",
        "            return_colbert_vecs=True\n",
        "        )\n",
        "        self.embeddings_BGE = self.model_encoder.encode(\n",
        "            self.single_contents_BGE,\n",
        "            return_dense=True,\n",
        "            return_sparse=True,\n",
        "            return_colbert_vecs=True\n",
        "        )\n",
        "        return self\n",
        "\n",
        "    @staticmethod\n",
        "    def tf(term, content):\n",
        "        if len(content.split()) == 0:\n",
        "            return 0\n",
        "        return content.split().count(term) / len(content.split())\n",
        "\n",
        "    @staticmethod\n",
        "    def idf(term, contents):\n",
        "        doc_count = sum(1 for content in contents if term in content.split())\n",
        "        return np.log(len(contents) / (1 + doc_count))\n",
        "\n",
        "    def get_scores1(self, query_embedded):\n",
        "        colbert_scores = np.array([\n",
        "            self.model_encoder.colbert_score(query_embedded['colbert_vecs'], self.embeddings_BGE['colbert_vecs'][i])\n",
        "            for i in range(len(self.embeddings_BGE['colbert_vecs']))\n",
        "        ])\n",
        "        sparse_scores = np.array([\n",
        "            self.model_encoder.compute_lexical_matching_score(query_embedded['lexical_weights'], self.embeddings_BGE['lexical_weights'][i])\n",
        "            for i in range(len(self.embeddings_BGE['lexical_weights']))\n",
        "        ])\n",
        "        dense_scores = np.array([\n",
        "            query_embedded['dense_vecs'].dot(self.embeddings_BGE['dense_vecs'][i])\n",
        "            for i in range(len(self.embeddings_BGE['dense_vecs']))\n",
        "        ])\n",
        "        return colbert_scores + dense_scores + sparse_scores\n",
        "\n",
        "    def get_scores2(self, query_embedded, query_keywords, query_processed):\n",
        "        keyword_weight = np.zeros(len(self.single_contents))\n",
        "        if query_keywords == \"\":\n",
        "            query_keywords = query_processed\n",
        "        for keyword in query_keywords.split():\n",
        "            for i in range(len(self.single_contents)):\n",
        "                similarity = cosine_similarity(\n",
        "                    query_embedded, self.embeddings['dense_vecs'][i].reshape(1, -1)\n",
        "                )[0, 0]\n",
        "                other_similarity = [\n",
        "                    cosine_similarity(\n",
        "                        self.embeddings['dense_vecs'][j].reshape(1, -1),\n",
        "                        self.embeddings['dense_vecs'][i].reshape(1, -1)\n",
        "                    )[0, 0]\n",
        "                    for j in range(len(self.single_contents)) if i != j\n",
        "                ]\n",
        "                if similarity > 0.1:\n",
        "                    keyword_weight[i] += (\n",
        "                        self.idf(keyword, self.single_contents)\n",
        "                        * self.tf(keyword, self.single_contents[i])\n",
        "                        * similarity\n",
        "                        / (1 - np.mean(other_similarity) + 1e-6)\n",
        "                    )\n",
        "        keyword_weight = (keyword_weight - np.min(keyword_weight)) / (np.max(keyword_weight) - np.min(keyword_weight))\n",
        "        return keyword_weight\n",
        "\n",
        "    def fit(self):\n",
        "        self.processor()\n",
        "        self.get_embeddings()\n",
        "\n",
        "    def make_response(self, query, alpha=0.1):\n",
        "        query_BGE = self.text_processing(query, process_stopwords=False)\n",
        "        query_embedding_BGE = self.model_encoder.encode(\n",
        "            query_BGE,\n",
        "            return_dense=True,\n",
        "            return_sparse=True,\n",
        "            return_colbert_vecs=True\n",
        "        )\n",
        "\n",
        "        query_processed = self.text_processing(query)\n",
        "        query_keywords = \" \".join(self.extractor(query_processed))\n",
        "        query_embedding = self.model_encoder.encode(\n",
        "            query_processed,\n",
        "            return_dense=True,\n",
        "            return_sparse=True,\n",
        "            return_colbert_vecs=True\n",
        "        )['dense_vecs'].reshape(1, -1)\n",
        "\n",
        "        scores1 = self.get_scores1(query_embedding_BGE)\n",
        "        scores2 = self.get_scores2(query_embedding, query_keywords, query_processed)\n",
        "        scores = scores1 + scores2\n",
        "\n",
        "        std_dev = np.std(scores)\n",
        "        mean_value = np.mean(scores)\n",
        "        thresholded_weights = [\n",
        "            (i, kw) for i, kw in enumerate(scores) if kw > (mean_value + alpha * std_dev)\n",
        "        ]\n",
        "        top_k_indices = sorted(thresholded_weights, key=lambda x: x[1], reverse=True)\n",
        "        top_k_indices = [i for i, _ in top_k_indices]\n",
        "        list_ans = [self.documents[i] for i in top_k_indices]\n",
        "        sentences = '. '.join(list_ans)\n",
        "\n",
        "        # API\n",
        "        genai.configure(api_key=\"AIzaSyDJXZAl17Pj-S66d_YofSQY7dDM39ynGUw\")\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "\n",
        "        # Prompt\n",
        "        prompt = f\"\"\"\n",
        "        This is a text containing information about a specific topic:\n",
        "\n",
        "        {sentences}\n",
        "\n",
        "        Based on this text, please answer the following question:\n",
        "\n",
        "        {query}\n",
        "\n",
        "        Please create a complete and clear answer, using knowledge only from the above text.\n",
        "        \"\"\"\n",
        "        response = model.generate_content(prompt)\n",
        "        result = response.text\n",
        "\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gmfo0t1VLoio"
      },
      "outputs": [],
      "source": [
        "model=MyModel(contents=contents)\n",
        "model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "jypLlhpIWXFn",
        "outputId": "3617ed06-4c1d-4822-807d-ab55369ff320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: The policy is a Privacy Policy that explains how the company collects, uses, and discloses information about its customers and visitors.  It will be posted on their website and may be updated from time to time.  The company commits to using personal information only for the purposes identified within the policy itself.  The company also monitors its data processing activities to ensure compliance with this policy and applicable laws, and will notify users and authorities in the event of a data breach.  Finally, they commit to cooperating with data protection authorities.\n",
            "\n",
            "Runining Time: 7.158730745315552 seconds\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "query = \"what is policy?\"\n",
        "\n",
        "print(f\"Answer: {model.make_response(query)}\")\n",
        "end = time.time()\n",
        "print(f\"Runining Time: {end - start} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "Cnbah2ONOAxA",
        "outputId": "b4c3a516-f8c6-4e66-94ce-080849ff6f02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer: The latest update to the Privacy Policy is 15 Sep 2023.\n",
            "\n",
            "Runining Time: 5.273918151855469 seconds\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "query = \"when is the latest update?\"\n",
        "\n",
        "print(f\"Answer: {model.make_response(query)}\")\n",
        "end = time.time()\n",
        "print(f\"Runining Time: {end - start} seconds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4txUodI7jHXu"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}